{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3331c805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade tensorflow \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glacierml as gl\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tensorflow.python.util import deprecation\n",
    "import os\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import janitor\n",
    "import tabulate\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59ec87af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3377835",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glam = Glam[[\n",
    "#         'LAT',\n",
    "#         'LON',\n",
    "    'CenLon',\n",
    "    'CenLat',\n",
    "    'Area',\n",
    "    'thickness',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Aspect',\n",
    "#     'Lmax'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca499e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CenLon</th>\n",
       "      <th>CenLat</th>\n",
       "      <th>Area</th>\n",
       "      <th>thickness</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>Zmed</th>\n",
       "      <th>Zmax</th>\n",
       "      <th>Aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.49600</td>\n",
       "      <td>67.91000</td>\n",
       "      <td>3.696</td>\n",
       "      <td>84.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>1207</td>\n",
       "      <td>1443</td>\n",
       "      <td>2070</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.56900</td>\n",
       "      <td>67.90300</td>\n",
       "      <td>3.405</td>\n",
       "      <td>99.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>1143</td>\n",
       "      <td>1414</td>\n",
       "      <td>1797</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-121.04816</td>\n",
       "      <td>48.35199</td>\n",
       "      <td>0.012</td>\n",
       "      <td>99.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>2058</td>\n",
       "      <td>2093</td>\n",
       "      <td>2128</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-117.28400</td>\n",
       "      <td>52.17540</td>\n",
       "      <td>16.154</td>\n",
       "      <td>150.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>1982</td>\n",
       "      <td>2870</td>\n",
       "      <td>3448</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-109.63833</td>\n",
       "      <td>43.17324</td>\n",
       "      <td>3.061</td>\n",
       "      <td>55.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>3328</td>\n",
       "      <td>3600</td>\n",
       "      <td>4052</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>14.14120</td>\n",
       "      <td>77.94450</td>\n",
       "      <td>5.943</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>234</td>\n",
       "      <td>338</td>\n",
       "      <td>663</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>14.06910</td>\n",
       "      <td>77.97140</td>\n",
       "      <td>6.737</td>\n",
       "      <td>65.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>195</td>\n",
       "      <td>336</td>\n",
       "      <td>521</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>37.35070</td>\n",
       "      <td>-3.05715</td>\n",
       "      <td>0.616</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5663</td>\n",
       "      <td>5771</td>\n",
       "      <td>5794</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>15.48540</td>\n",
       "      <td>77.02720</td>\n",
       "      <td>0.418</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>323</td>\n",
       "      <td>422</td>\n",
       "      <td>587</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>12.16140</td>\n",
       "      <td>78.87100</td>\n",
       "      <td>5.016</td>\n",
       "      <td>76.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>123</td>\n",
       "      <td>388</td>\n",
       "      <td>684</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CenLon    CenLat    Area  thickness  Slope  Zmin  Zmed  Zmax  Aspect\n",
       "0     18.49600  67.91000   3.696       84.0   15.9  1207  1443  2070     282\n",
       "1     18.56900  67.90300   3.405       99.0   13.2  1143  1414  1797     102\n",
       "2   -121.04816  48.35199   0.012       99.0   27.4  2058  2093  2128     268\n",
       "3   -117.28400  52.17540  16.154      150.0   12.6  1982  2870  3448      93\n",
       "4   -109.63833  43.17324   3.061       55.0   19.4  3328  3600  4052      26\n",
       "..         ...       ...     ...        ...    ...   ...   ...   ...     ...\n",
       "304   14.14120  77.94450   5.943       11.0   11.9   234   338   663      69\n",
       "305   14.06910  77.97140   6.737       65.0   10.1   195   336   521      56\n",
       "306   37.35070  -3.05715   0.616       23.0    9.9  5663  5771  5794     355\n",
       "307   15.48540  77.02720   0.418       27.0   18.8   323   422   587     144\n",
       "308   12.16140  78.87100   5.016       76.0   13.8   123   388   684     350\n",
       "\n",
       "[309 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Glam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec245a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glam = pd.merge(Glam, RGI, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdf9b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glam.to_csv('Glam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02244433",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/zults/RGI_prethicked.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4034670/794227375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m'Lmax'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m ]]\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mRGI_prethicked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/zults/RGI_prethicked.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m RGI_prethicked = RGI_prethicked[[\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         'LAT',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/zults/RGI_prethicked.csv'"
     ]
    }
   ],
   "source": [
    "Glam = pd.read_csv('Glam.csv')\n",
    "Glam = Glam[[\n",
    "#         'LAT',\n",
    "#         'LON',\n",
    "    'CenLon',\n",
    "    'CenLat',\n",
    "    'Area',\n",
    "    'thickness',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Aspect',\n",
    "    'Lmax'\n",
    "]]\n",
    "RGI_prethicked = pd.read_csv('zults/RGI_prethicked.csv')\n",
    "RGI_prethicked = RGI_prethicked[[\n",
    "#         'LAT',\n",
    "#         'LON',\n",
    "    'CenLon',\n",
    "    'CenLat',\n",
    "    'Area',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Aspect',\n",
    "    'Lmax',\n",
    "    'avg predicted thickness',\n",
    "    'predicted thickness std dev',\n",
    "    'variance'\n",
    "]]\n",
    "deviations = pd.read_csv('deviations.csv')\n",
    "deviations = deviations[[\n",
    "#         'LAT',\n",
    "#         'LON',\n",
    "    'layer architecture',\n",
    "    'model parameters',\n",
    "    'learning rate',\n",
    "    'validation split',\n",
    "    'test mae avg',\n",
    "    'train mae avg',\n",
    "    'test mae std dev',\n",
    "    'train mae std dev',\n",
    "    'test predicted thickness std dev',\n",
    "    'train predicted thickness std dev'\n",
    "]]\n",
    "\n",
    "# Glam = pd.read_csv('Glam.csv')\n",
    "# Glam = pd.read_csv('Glam.csv')\n",
    "# Glam = pd.read_csv('Glam.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110ab895",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CenLon</th>\n",
       "      <th>CenLat</th>\n",
       "      <th>Area</th>\n",
       "      <th>thickness</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>Zmed</th>\n",
       "      <th>Zmax</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Lmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-145.4270</td>\n",
       "      <td>63.2810</td>\n",
       "      <td>17.567</td>\n",
       "      <td>147.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1162</td>\n",
       "      <td>1858</td>\n",
       "      <td>2438</td>\n",
       "      <td>172</td>\n",
       "      <td>8639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-134.3490</td>\n",
       "      <td>58.3800</td>\n",
       "      <td>9.528</td>\n",
       "      <td>66.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>676</td>\n",
       "      <td>1123</td>\n",
       "      <td>1494</td>\n",
       "      <td>327</td>\n",
       "      <td>6332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-143.8190</td>\n",
       "      <td>69.2760</td>\n",
       "      <td>0.153</td>\n",
       "      <td>82.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2360</td>\n",
       "      <td>2440</td>\n",
       "      <td>2554</td>\n",
       "      <td>219</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-140.3020</td>\n",
       "      <td>61.2000</td>\n",
       "      <td>5.859</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2145</td>\n",
       "      <td>2509</td>\n",
       "      <td>2894</td>\n",
       "      <td>341</td>\n",
       "      <td>3374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-116.3300</td>\n",
       "      <td>51.1770</td>\n",
       "      <td>0.872</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>2390</td>\n",
       "      <td>2526</td>\n",
       "      <td>2853</td>\n",
       "      <td>69</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-70.2819</td>\n",
       "      <td>-33.1827</td>\n",
       "      <td>0.164</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31.1</td>\n",
       "      <td>4142</td>\n",
       "      <td>4297</td>\n",
       "      <td>4446</td>\n",
       "      <td>123</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-70.1014</td>\n",
       "      <td>-33.0259</td>\n",
       "      <td>0.022</td>\n",
       "      <td>59.0</td>\n",
       "      <td>41.3</td>\n",
       "      <td>4645</td>\n",
       "      <td>4782</td>\n",
       "      <td>4898</td>\n",
       "      <td>64</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-69.9276</td>\n",
       "      <td>-30.1454</td>\n",
       "      <td>1.189</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4664</td>\n",
       "      <td>5187</td>\n",
       "      <td>5525</td>\n",
       "      <td>148</td>\n",
       "      <td>1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-69.7318</td>\n",
       "      <td>-28.5387</td>\n",
       "      <td>3.140</td>\n",
       "      <td>39.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4784</td>\n",
       "      <td>5234</td>\n",
       "      <td>5597</td>\n",
       "      <td>190</td>\n",
       "      <td>3349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-69.6091</td>\n",
       "      <td>-28.3814</td>\n",
       "      <td>5.209</td>\n",
       "      <td>67.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>5068</td>\n",
       "      <td>5406</td>\n",
       "      <td>5753</td>\n",
       "      <td>79</td>\n",
       "      <td>3782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CenLon   CenLat    Area  thickness  Slope  Zmin  Zmed  Zmax  Aspect  \\\n",
       "0   -145.4270  63.2810  17.567      147.0   14.0  1162  1858  2438     172   \n",
       "1   -134.3490  58.3800   9.528       66.0   10.0   676  1123  1494     327   \n",
       "2   -143.8190  69.2760   0.153       82.0   22.0  2360  2440  2554     219   \n",
       "3   -140.3020  61.2000   5.859       66.0   14.0  2145  2509  2894     341   \n",
       "4   -116.3300  51.1770   0.872       70.0   15.3  2390  2526  2853      69   \n",
       "..        ...      ...     ...        ...    ...   ...   ...   ...     ...   \n",
       "301  -70.2819 -33.1827   0.164       16.0   31.1  4142  4297  4446     123   \n",
       "302  -70.1014 -33.0259   0.022       59.0   41.3  4645  4782  4898      64   \n",
       "303  -69.9276 -30.1454   1.189       28.0   29.3  4664  5187  5525     148   \n",
       "304  -69.7318 -28.5387   3.140       39.0   16.0  4784  5234  5597     190   \n",
       "305  -69.6091 -28.3814   5.209       67.0   11.6  5068  5406  5753      79   \n",
       "\n",
       "     Lmax  \n",
       "0    8639  \n",
       "1    6332  \n",
       "2     680  \n",
       "3    3374  \n",
       "4    1181  \n",
       "..    ...  \n",
       "301   712  \n",
       "302   280  \n",
       "303  1596  \n",
       "304  3349  \n",
       "305  3782  \n",
       "\n",
       "[306 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Glam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444bce02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CenLon</th>\n",
       "      <th>CenLat</th>\n",
       "      <th>Area</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>Zmed</th>\n",
       "      <th>Zmax</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Lmax</th>\n",
       "      <th>avg predicted thickness</th>\n",
       "      <th>predicted thickness std dev</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-146.823</td>\n",
       "      <td>63.6890</td>\n",
       "      <td>0.360</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>2385</td>\n",
       "      <td>2725</td>\n",
       "      <td>346</td>\n",
       "      <td>839</td>\n",
       "      <td>1289.063672</td>\n",
       "      <td>278.267904</td>\n",
       "      <td>77433.026643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-146.668</td>\n",
       "      <td>63.4040</td>\n",
       "      <td>0.558</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1713</td>\n",
       "      <td>2005</td>\n",
       "      <td>2144</td>\n",
       "      <td>162</td>\n",
       "      <td>1197</td>\n",
       "      <td>1158.236094</td>\n",
       "      <td>241.924732</td>\n",
       "      <td>58527.575944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-146.080</td>\n",
       "      <td>63.3760</td>\n",
       "      <td>1.685</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1609</td>\n",
       "      <td>1868</td>\n",
       "      <td>2182</td>\n",
       "      <td>175</td>\n",
       "      <td>2106</td>\n",
       "      <td>1083.203828</td>\n",
       "      <td>227.856348</td>\n",
       "      <td>51918.515428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-146.120</td>\n",
       "      <td>63.3810</td>\n",
       "      <td>3.681</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1273</td>\n",
       "      <td>1944</td>\n",
       "      <td>2317</td>\n",
       "      <td>195</td>\n",
       "      <td>4175</td>\n",
       "      <td>845.136094</td>\n",
       "      <td>181.513650</td>\n",
       "      <td>32947.205208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-147.057</td>\n",
       "      <td>63.5510</td>\n",
       "      <td>2.573</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1494</td>\n",
       "      <td>1914</td>\n",
       "      <td>2317</td>\n",
       "      <td>181</td>\n",
       "      <td>2981</td>\n",
       "      <td>1004.057188</td>\n",
       "      <td>211.500073</td>\n",
       "      <td>44732.280676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208310</th>\n",
       "      <td>170.354</td>\n",
       "      <td>-43.4215</td>\n",
       "      <td>0.189</td>\n",
       "      <td>34.7</td>\n",
       "      <td>1231</td>\n",
       "      <td>1724</td>\n",
       "      <td>2098</td>\n",
       "      <td>116</td>\n",
       "      <td>944</td>\n",
       "      <td>858.659844</td>\n",
       "      <td>181.499115</td>\n",
       "      <td>32941.928915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208311</th>\n",
       "      <td>170.349</td>\n",
       "      <td>-43.4550</td>\n",
       "      <td>0.040</td>\n",
       "      <td>36.1</td>\n",
       "      <td>1881</td>\n",
       "      <td>2106</td>\n",
       "      <td>2208</td>\n",
       "      <td>108</td>\n",
       "      <td>331</td>\n",
       "      <td>1315.604219</td>\n",
       "      <td>272.310822</td>\n",
       "      <td>74153.183973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208312</th>\n",
       "      <td>170.351</td>\n",
       "      <td>-43.4400</td>\n",
       "      <td>0.184</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1677</td>\n",
       "      <td>1974</td>\n",
       "      <td>2253</td>\n",
       "      <td>104</td>\n",
       "      <td>740</td>\n",
       "      <td>1168.286641</td>\n",
       "      <td>244.716836</td>\n",
       "      <td>59886.330066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208313</th>\n",
       "      <td>170.364</td>\n",
       "      <td>-43.4106</td>\n",
       "      <td>0.111</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1627</td>\n",
       "      <td>1839</td>\n",
       "      <td>1928</td>\n",
       "      <td>135</td>\n",
       "      <td>406</td>\n",
       "      <td>1139.257578</td>\n",
       "      <td>236.082631</td>\n",
       "      <td>55735.008786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208314</th>\n",
       "      <td>170.323</td>\n",
       "      <td>-43.3829</td>\n",
       "      <td>0.085</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1837</td>\n",
       "      <td>1927</td>\n",
       "      <td>1992</td>\n",
       "      <td>207</td>\n",
       "      <td>292</td>\n",
       "      <td>1292.534531</td>\n",
       "      <td>263.953169</td>\n",
       "      <td>69671.275219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208315 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CenLon   CenLat   Area  Slope  Zmin  Zmed  Zmax  Aspect  Lmax  \\\n",
       "0      -146.823  63.6890  0.360   42.0  1936  2385  2725     346   839   \n",
       "1      -146.668  63.4040  0.558   16.0  1713  2005  2144     162  1197   \n",
       "2      -146.080  63.3760  1.685   18.0  1609  1868  2182     175  2106   \n",
       "3      -146.120  63.3810  3.681   19.0  1273  1944  2317     195  4175   \n",
       "4      -147.057  63.5510  2.573   16.0  1494  1914  2317     181  2981   \n",
       "...         ...      ...    ...    ...   ...   ...   ...     ...   ...   \n",
       "208310  170.354 -43.4215  0.189   34.7  1231  1724  2098     116   944   \n",
       "208311  170.349 -43.4550  0.040   36.1  1881  2106  2208     108   331   \n",
       "208312  170.351 -43.4400  0.184   39.2  1677  1974  2253     104   740   \n",
       "208313  170.364 -43.4106  0.111   34.0  1627  1839  1928     135   406   \n",
       "208314  170.323 -43.3829  0.085   30.1  1837  1927  1992     207   292   \n",
       "\n",
       "        avg predicted thickness  predicted thickness std dev      variance  \n",
       "0                   1289.063672                   278.267904  77433.026643  \n",
       "1                   1158.236094                   241.924732  58527.575944  \n",
       "2                   1083.203828                   227.856348  51918.515428  \n",
       "3                    845.136094                   181.513650  32947.205208  \n",
       "4                   1004.057188                   211.500073  44732.280676  \n",
       "...                         ...                          ...           ...  \n",
       "208310               858.659844                   181.499115  32941.928915  \n",
       "208311              1315.604219                   272.310822  74153.183973  \n",
       "208312              1168.286641                   244.716836  59886.330066  \n",
       "208313              1139.257578                   236.082631  55735.008786  \n",
       "208314              1292.534531                   263.953169  69671.275219  \n",
       "\n",
       "[208315 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RGI_prethicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07006757",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer architecture</th>\n",
       "      <th>model parameters</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>validation split</th>\n",
       "      <th>test mae avg</th>\n",
       "      <th>train mae avg</th>\n",
       "      <th>test mae std dev</th>\n",
       "      <th>train mae std dev</th>\n",
       "      <th>test predicted thickness std dev</th>\n",
       "      <th>train predicted thickness std dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32-16-8</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.207832</td>\n",
       "      <td>13.195737</td>\n",
       "      <td>0.325927</td>\n",
       "      <td>0.427031</td>\n",
       "      <td>0.591357</td>\n",
       "      <td>0.486747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24-12-6</td>\n",
       "      <td>644.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.415105</td>\n",
       "      <td>13.689977</td>\n",
       "      <td>0.410488</td>\n",
       "      <td>0.380411</td>\n",
       "      <td>0.567290</td>\n",
       "      <td>0.473328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12-8</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.857983</td>\n",
       "      <td>14.885575</td>\n",
       "      <td>0.274563</td>\n",
       "      <td>0.286594</td>\n",
       "      <td>0.592051</td>\n",
       "      <td>0.572013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16-8</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.882614</td>\n",
       "      <td>14.671638</td>\n",
       "      <td>0.255216</td>\n",
       "      <td>0.218825</td>\n",
       "      <td>0.364422</td>\n",
       "      <td>0.352042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-6</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.949541</td>\n",
       "      <td>15.135346</td>\n",
       "      <td>0.284604</td>\n",
       "      <td>0.285441</td>\n",
       "      <td>0.520103</td>\n",
       "      <td>0.430230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10-5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.091524</td>\n",
       "      <td>15.287114</td>\n",
       "      <td>0.382744</td>\n",
       "      <td>0.327079</td>\n",
       "      <td>0.488009</td>\n",
       "      <td>0.434573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12-6</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.258951</td>\n",
       "      <td>12.697351</td>\n",
       "      <td>0.994131</td>\n",
       "      <td>0.708047</td>\n",
       "      <td>1.165205</td>\n",
       "      <td>0.954331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10-5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.308621</td>\n",
       "      <td>12.831752</td>\n",
       "      <td>0.602335</td>\n",
       "      <td>0.627691</td>\n",
       "      <td>0.797038</td>\n",
       "      <td>0.746737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16-8</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.468252</td>\n",
       "      <td>11.856180</td>\n",
       "      <td>0.910006</td>\n",
       "      <td>0.837275</td>\n",
       "      <td>0.966888</td>\n",
       "      <td>0.882962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8-4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.658732</td>\n",
       "      <td>13.962586</td>\n",
       "      <td>3.111561</td>\n",
       "      <td>3.332467</td>\n",
       "      <td>4.761145</td>\n",
       "      <td>4.271080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12-8</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.014602</td>\n",
       "      <td>13.091746</td>\n",
       "      <td>3.247421</td>\n",
       "      <td>3.549813</td>\n",
       "      <td>4.827886</td>\n",
       "      <td>4.406695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10-5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.284520</td>\n",
       "      <td>11.670497</td>\n",
       "      <td>1.606948</td>\n",
       "      <td>1.948284</td>\n",
       "      <td>1.970232</td>\n",
       "      <td>1.917905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32-16-8</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.459327</td>\n",
       "      <td>10.380450</td>\n",
       "      <td>1.225341</td>\n",
       "      <td>1.412582</td>\n",
       "      <td>4.272036</td>\n",
       "      <td>3.714475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8-4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.536946</td>\n",
       "      <td>12.728193</td>\n",
       "      <td>2.003514</td>\n",
       "      <td>2.384888</td>\n",
       "      <td>2.387979</td>\n",
       "      <td>2.266510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8-4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.579442</td>\n",
       "      <td>16.821427</td>\n",
       "      <td>6.111449</td>\n",
       "      <td>6.122259</td>\n",
       "      <td>7.824829</td>\n",
       "      <td>7.255960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12-8</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.750306</td>\n",
       "      <td>10.798886</td>\n",
       "      <td>1.114175</td>\n",
       "      <td>0.791291</td>\n",
       "      <td>2.502507</td>\n",
       "      <td>1.924764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24-12-6</td>\n",
       "      <td>644.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.784888</td>\n",
       "      <td>9.602571</td>\n",
       "      <td>1.403465</td>\n",
       "      <td>0.777434</td>\n",
       "      <td>1.707636</td>\n",
       "      <td>1.159368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12-6</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.925964</td>\n",
       "      <td>10.796466</td>\n",
       "      <td>1.243910</td>\n",
       "      <td>1.232901</td>\n",
       "      <td>2.634162</td>\n",
       "      <td>2.056257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16-8</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.166462</td>\n",
       "      <td>9.875847</td>\n",
       "      <td>1.506537</td>\n",
       "      <td>0.981434</td>\n",
       "      <td>2.400590</td>\n",
       "      <td>1.982836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24-12-6</td>\n",
       "      <td>644.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.313205</td>\n",
       "      <td>10.481938</td>\n",
       "      <td>1.425660</td>\n",
       "      <td>1.303758</td>\n",
       "      <td>2.894681</td>\n",
       "      <td>2.574913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32-16-8</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.320735</td>\n",
       "      <td>8.739548</td>\n",
       "      <td>1.129238</td>\n",
       "      <td>0.922486</td>\n",
       "      <td>1.347646</td>\n",
       "      <td>0.952754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer architecture  model parameters  learning rate  validation split  \\\n",
       "0             32-16-8            1012.0          0.001               0.2   \n",
       "1             24-12-6             644.0          0.001               0.2   \n",
       "2                12-8             252.0          0.001               0.2   \n",
       "3                16-8             324.0          0.001               0.2   \n",
       "4                12-6             224.0          0.001               0.2   \n",
       "5                10-5             180.0          0.001               0.2   \n",
       "6                12-6             224.0          0.010               0.2   \n",
       "7                10-5             180.0          0.010               0.2   \n",
       "8                16-8             324.0          0.010               0.2   \n",
       "9                 8-4             140.0          0.010               0.2   \n",
       "10               12-8             252.0          0.010               0.2   \n",
       "11               10-5             180.0          0.100               0.2   \n",
       "12            32-16-8            1012.0          0.100               0.2   \n",
       "13                8-4             140.0          0.100               0.2   \n",
       "14                8-4             140.0          0.001               0.2   \n",
       "15               12-8             252.0          0.100               0.2   \n",
       "16            24-12-6             644.0          0.010               0.2   \n",
       "17               12-6             224.0          0.100               0.2   \n",
       "18               16-8             324.0          0.100               0.2   \n",
       "19            24-12-6             644.0          0.100               0.2   \n",
       "20            32-16-8            1012.0          0.010               0.2   \n",
       "\n",
       "    test mae avg  train mae avg  test mae std dev  train mae std dev  \\\n",
       "0      12.207832      13.195737          0.325927           0.427031   \n",
       "1      12.415105      13.689977          0.410488           0.380411   \n",
       "2      12.857983      14.885575          0.274563           0.286594   \n",
       "3      12.882614      14.671638          0.255216           0.218825   \n",
       "4      12.949541      15.135346          0.284604           0.285441   \n",
       "5      13.091524      15.287114          0.382744           0.327079   \n",
       "6      13.258951      12.697351          0.994131           0.708047   \n",
       "7      13.308621      12.831752          0.602335           0.627691   \n",
       "8      13.468252      11.856180          0.910006           0.837275   \n",
       "9      13.658732      13.962586          3.111561           3.332467   \n",
       "10     14.014602      13.091746          3.247421           3.549813   \n",
       "11     14.284520      11.670497          1.606948           1.948284   \n",
       "12     14.459327      10.380450          1.225341           1.412582   \n",
       "13     14.536946      12.728193          2.003514           2.384888   \n",
       "14     14.579442      16.821427          6.111449           6.122259   \n",
       "15     14.750306      10.798886          1.114175           0.791291   \n",
       "16     14.784888       9.602571          1.403465           0.777434   \n",
       "17     14.925964      10.796466          1.243910           1.232901   \n",
       "18     15.166462       9.875847          1.506537           0.981434   \n",
       "19     15.313205      10.481938          1.425660           1.303758   \n",
       "20     15.320735       8.739548          1.129238           0.922486   \n",
       "\n",
       "    test predicted thickness std dev  train predicted thickness std dev  \n",
       "0                           0.591357                           0.486747  \n",
       "1                           0.567290                           0.473328  \n",
       "2                           0.592051                           0.572013  \n",
       "3                           0.364422                           0.352042  \n",
       "4                           0.520103                           0.430230  \n",
       "5                           0.488009                           0.434573  \n",
       "6                           1.165205                           0.954331  \n",
       "7                           0.797038                           0.746737  \n",
       "8                           0.966888                           0.882962  \n",
       "9                           4.761145                           4.271080  \n",
       "10                          4.827886                           4.406695  \n",
       "11                          1.970232                           1.917905  \n",
       "12                          4.272036                           3.714475  \n",
       "13                          2.387979                           2.266510  \n",
       "14                          7.824829                           7.255960  \n",
       "15                          2.502507                           1.924764  \n",
       "16                          1.707636                           1.159368  \n",
       "17                          2.634162                           2.056257  \n",
       "18                          2.400590                           1.982836  \n",
       "19                          2.894681                           2.574913  \n",
       "20                          1.347646                           0.952754  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9fa5dfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ccccombo_breaker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2793263/3289444459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mccccombo_breaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# past this point is under construction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# here be monsters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ccccombo_breaker' is not defined"
     ]
    }
   ],
   "source": [
    "ccccombo_breaker()\n",
    "# past this point is under construction. \n",
    "# here be monsters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61500555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19a3c61",
   "metadata": {},
   "source": [
    "# Step 1.\n",
    "### Load and define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b11ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glam = pd.read_csv('Glam.csv')\n",
    "Glam_phys = Glam[[\n",
    "#         'LAT',\n",
    "#         'LON',\n",
    "#     'CenLon',\n",
    "#     'CenLat',\n",
    "    'Area',\n",
    "    'thickness',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "]]\n",
    "# split data for training and validation\n",
    "# (train_features, test_features, train_labels, test_labels) = gl.data_splitter(Glam)\n",
    "\n",
    "# define model hyperparameters\n",
    "RS = range(0,25,1)\n",
    "ep = 300\n",
    "\n",
    "# name databases\n",
    "Glam_phys.name = 'Glam_phys'\n",
    "\n",
    "\n",
    "\n",
    "# split data for training and validation\n",
    "(train_features, test_features, train_labels, test_labels) = gl.data_splitter(Glam_phys)\n",
    "\n",
    "# define model hyperparameters\n",
    "RS = range(0,25,1)\n",
    "ep = 300\n",
    "\n",
    "# name databases\n",
    "Glam.name = 'Glam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9921e",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "### Load and models, then make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd4553b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glam_phys_dnn_MULTI_0.1_0.2_300_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1557, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1546, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1535, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1499, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"normalization_7\" (type Normalization).\n    \n    Dimensions must be equal, but are 5 and 7 for '{{node sequential/normalization_7/sub}} = Sub[T=DT_FLOAT](sequential/Cast, sequential/normalization_7/sub/y)' with input shapes: [?,5], [1,7].\n    \n    Call arguments received by layer \"normalization_7\" (type Normalization):\n      • inputs=tf.Tensor(shape=(None, 5), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2793263/246330473.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     + folder)\n\u001b[1;32m     15\u001b[0m                 mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n\u001b[0;32m---> 16\u001b[0;31m                                                              test_labels,verbose=0)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1557, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1546, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1535, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/training.py\", line 1499, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"normalization_7\" (type Normalization).\n    \n    Dimensions must be equal, but are 5 and 7 for '{{node sequential/normalization_7/sub}} = Sub[T=DT_FLOAT](sequential/Cast, sequential/normalization_7/sub/y)' with input shapes: [?,5], [1,7].\n    \n    Call arguments received by layer \"normalization_7\" (type Normalization):\n      • inputs=tf.Tensor(shape=(None, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rootdir = 'sm4/'\n",
    "# print(rootdir)\n",
    "dnn_model={}\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder and 'dnn' in folder:\n",
    "            print(folder)\n",
    "            if '0.1' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.1'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'\n",
    "                \n",
    "            if '0.01' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features, verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.01'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'          \n",
    "            \n",
    "            if '0.001' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]            \n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.001'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'                \n",
    "                \n",
    "predictions.rename(columns = {0:'avg train thickness'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c7bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we evaluate models and make predictions, then display the zults\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# these models are ridiculous, so we drop them.\n",
    "# idx = predictions.index[predictions['architecture']=='64']\n",
    "# predictions = predictions.drop(predictions.loc[idx].index)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we compute for each layer architecture avg mae, mae std dev, and\n",
    "prediction std dev.\n",
    "\"\"\"\n",
    "deviations = pd.DataFrame()\n",
    "for architecture in list(predictions['architecture'].unique()):\n",
    "    for learning_rate in list(predictions['learning rate'].unique()):\n",
    "        # define temp dataframe for calculations that contains only one layer architecture\n",
    "#         df = (predictions[predictions['architecture'] == architecture]) and (predictions[predictions['learning rate'] == str(learning_rate)])\n",
    "        df = predictions[(predictions['architecture'] == architecture) & (predictions['learning rate' ]== learning_rate)]\n",
    "#         break\n",
    "#         print(df)\n",
    "        # step 1: calculate mean of numbers\n",
    "        test_mae_mean = np.sum(df['test mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for test_mae in df['test mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((test_mae - test_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        test_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "       # repeat for train mae \n",
    "\n",
    "        # step 1: calculate mean of numbers\n",
    "        train_mae_mean = np.sum(df['train mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for train_mae in df['train mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((train_mae - train_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        train_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # repeat process for train thicknesses\n",
    "        thickness_train_mean = np.sum(df['avg train thickness']) / len(df)   \n",
    "        for thickness in df['avg train thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_train_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        train_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "        # repeat process for test thicknesses\n",
    "        thickness_test_mean = np.sum(df['avg test thickness']) / len(df)   \n",
    "        for thickness in df['avg test thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_test_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        test_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # turn the last number computed into a series so it may be appended to build the table.\n",
    "        # it will be dropped later, no worries.\n",
    "        test_thick_std_dev = pd.Series(test_thickness_std_dev)\n",
    "\n",
    "        deviations = deviations.append(test_thick_std_dev, ignore_index=True)\n",
    "        deviations.loc[deviations.index[-1], 'layer architecture']= architecture\n",
    "        deviations.loc[deviations.index[-1], 'model parameters'] = dnn_model[architecture + '_Glam_dnn_MULTI_0.1_0.2_300_0'].count_params()\n",
    "        deviations.loc[deviations.index[-1], 'learning rate']= learning_rate\n",
    "        deviations.loc[deviations.index[-1], 'validation split']= 0.2\n",
    "        deviations.loc[deviations.index[-1], 'test mae avg'] = test_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'train mae avg']= train_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'test mae std dev']= test_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train mae std dev']= train_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'test predicted thickness std dev']= test_thickness_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train predicted thickness std dev']= train_thickness_std_dev\n",
    "# bootstrapped ensembles for predicted column    \n",
    "#drop that appended line from earlier. Probably a better way to go about it\n",
    "deviations.drop(columns = {0},inplace = True)    \n",
    "deviations = deviations.dropna()\n",
    "# deviations['training split'] = deviations['test mae avg'] - deviations['train mae avg']\n",
    "# too_low = deviations.index[deviations['training split'] < 0]\n",
    "# deviations = deviations.drop(too_low)\n",
    "deviations = deviations.sort_values('test mae avg')\n",
    "deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbae31",
   "metadata": {},
   "source": [
    "# Build RGI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "544e329a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CenLat</th>\n",
       "      <th>CenLon</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>Zmed</th>\n",
       "      <th>Zmax</th>\n",
       "      <th>Area</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Lmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.6890</td>\n",
       "      <td>-146.823</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>2385</td>\n",
       "      <td>2725</td>\n",
       "      <td>0.360</td>\n",
       "      <td>346</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.4040</td>\n",
       "      <td>-146.668</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1713</td>\n",
       "      <td>2005</td>\n",
       "      <td>2144</td>\n",
       "      <td>0.558</td>\n",
       "      <td>162</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.3760</td>\n",
       "      <td>-146.080</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1609</td>\n",
       "      <td>1868</td>\n",
       "      <td>2182</td>\n",
       "      <td>1.685</td>\n",
       "      <td>175</td>\n",
       "      <td>2106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.3810</td>\n",
       "      <td>-146.120</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1273</td>\n",
       "      <td>1944</td>\n",
       "      <td>2317</td>\n",
       "      <td>3.681</td>\n",
       "      <td>195</td>\n",
       "      <td>4175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.5510</td>\n",
       "      <td>-147.057</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1494</td>\n",
       "      <td>1914</td>\n",
       "      <td>2317</td>\n",
       "      <td>2.573</td>\n",
       "      <td>181</td>\n",
       "      <td>2981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208310</th>\n",
       "      <td>-43.4215</td>\n",
       "      <td>170.354</td>\n",
       "      <td>34.7</td>\n",
       "      <td>1231</td>\n",
       "      <td>1724</td>\n",
       "      <td>2098</td>\n",
       "      <td>0.189</td>\n",
       "      <td>116</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208311</th>\n",
       "      <td>-43.4550</td>\n",
       "      <td>170.349</td>\n",
       "      <td>36.1</td>\n",
       "      <td>1881</td>\n",
       "      <td>2106</td>\n",
       "      <td>2208</td>\n",
       "      <td>0.040</td>\n",
       "      <td>108</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208312</th>\n",
       "      <td>-43.4400</td>\n",
       "      <td>170.351</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1677</td>\n",
       "      <td>1974</td>\n",
       "      <td>2253</td>\n",
       "      <td>0.184</td>\n",
       "      <td>104</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208313</th>\n",
       "      <td>-43.4106</td>\n",
       "      <td>170.364</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1627</td>\n",
       "      <td>1839</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.111</td>\n",
       "      <td>135</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208314</th>\n",
       "      <td>-43.3829</td>\n",
       "      <td>170.323</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1837</td>\n",
       "      <td>1927</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.085</td>\n",
       "      <td>207</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208315 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CenLat   CenLon  Slope  Zmin  Zmed  Zmax   Area  Aspect  Lmax\n",
       "0       63.6890 -146.823   42.0  1936  2385  2725  0.360     346   839\n",
       "1       63.4040 -146.668   16.0  1713  2005  2144  0.558     162  1197\n",
       "2       63.3760 -146.080   18.0  1609  1868  2182  1.685     175  2106\n",
       "3       63.3810 -146.120   19.0  1273  1944  2317  3.681     195  4175\n",
       "4       63.5510 -147.057   16.0  1494  1914  2317  2.573     181  2981\n",
       "...         ...      ...    ...   ...   ...   ...    ...     ...   ...\n",
       "208310 -43.4215  170.354   34.7  1231  1724  2098  0.189     116   944\n",
       "208311 -43.4550  170.349   36.1  1881  2106  2208  0.040     108   331\n",
       "208312 -43.4400  170.351   39.2  1677  1974  2253  0.184     104   740\n",
       "208313 -43.4106  170.364   34.0  1627  1839  1928  0.111     135   406\n",
       "208314 -43.3829  170.323   30.1  1837  1927  1992  0.085     207   292\n",
       "\n",
       "[208315 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_zmed = RGI.loc[RGI['Zmed']<0].index\n",
    "RGI = RGI.drop(bad_zmed)\n",
    "bad_lmax = RGI.loc[RGI['Lmax']<0].index\n",
    "RGI = RGI.drop(bad_lmax)\n",
    "bad_slope = RGI.loc[RGI['Slope']<0].index\n",
    "RGI = RGI.drop(bad_slope)\n",
    "bad_aspect = RGI.loc[RGI['Aspect']<0].index\n",
    "RGI = RGI.drop(bad_aspect)\n",
    "RGI = RGI.reset_index()\n",
    "RGI = RGI.drop('index', axis=1)\n",
    "RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b8e95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/data/fast0/datasets/rgi60-attribs/'\n",
    "RGI_extra = pd.DataFrame()\n",
    "for file in os.listdir(rootdir):\n",
    "#     print(file)\n",
    "    f = pd.read_csv(rootdir+file, encoding_errors = 'replace', on_bad_lines = 'skip')\n",
    "    RGI_extra = RGI_extra.append(f, ignore_index = True)\n",
    "\n",
    "RGI = RGI_extra[[\n",
    "    'CenLat',\n",
    "    'CenLon',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Area',\n",
    "    'Aspect',\n",
    "    'Lmax'\n",
    "]]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b39c9ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deviations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4034670/2810555809.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeviations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer architecture'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeviations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeviations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deviations' is not defined"
     ]
    }
   ],
   "source": [
    "rootdir = '/data/fast0/datasets/rgi60-attribs/'\n",
    "RGI_extra = pd.DataFrame()\n",
    "for file in os.listdir(rootdir):\n",
    "#     print(file)\n",
    "    f = pd.read_csv(rootdir+file, encoding_errors = 'replace', on_bad_lines = 'skip')\n",
    "    RGI_extra = RGI_extra.append(f, ignore_index = True)\n",
    "\n",
    "RGI = RGI_extra[[\n",
    "    'CenLat',\n",
    "    'CenLon',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Area',\n",
    "    'Aspect',\n",
    "    'Lmax'\n",
    "]]    \n",
    "\n",
    "rootdir = '/data/fast0/datasets/rgi60-attribs/'\n",
    "RGI_extra = pd.DataFrame()\n",
    "for file in os.listdir(rootdir):\n",
    "#     print(file)\n",
    "    f = pd.read_csv(rootdir+file, encoding_errors = 'replace', on_bad_lines = 'skip')\n",
    "    RGI_extra = RGI_extra.append(f, ignore_index = True)\n",
    "\n",
    "RGI = RGI_extra[[\n",
    "    'CenLat',\n",
    "    'CenLon',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Area',\n",
    "    'Aspect',\n",
    "    'Lmax'\n",
    "]]\n",
    "bad_zmed = RGI.loc[RGI['Zmed']<0].index\n",
    "RGI = RGI.drop(bad_zmed)\n",
    "bad_lmax = RGI.loc[RGI['Lmax']<0].index\n",
    "RGI = RGI.drop(bad_lmax)\n",
    "bad_slope = RGI.loc[RGI['Slope']<0].index\n",
    "RGI = RGI.drop(bad_slope)\n",
    "bad_aspect = RGI.loc[RGI['Aspect']<0].index\n",
    "RGI = RGI.drop(bad_aspect)\n",
    "RGI = RGI.reset_index()\n",
    "RGI = RGI.drop('index', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "arch = deviations['layer architecture'].iloc[0]\n",
    "lr = deviations['learning rate'].iloc[0]\n",
    "vs = deviations['validation split'].iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we predict thicknesses across all 25 of the top performing ensemble model\n",
    "\"\"\"\n",
    "# dfs = pd.DataFrame()\n",
    "# for rs in tqdm(RS):\n",
    "#     df = deviations.iloc[:1]\n",
    "#     s = pd.Series(dnn_model[str(arch) + '_Glam_dnn_MULTI_' + str(lr) + '_' + str(vs) + '_' +str(ep) + '_' + str(rs)].predict(RGI, verbose=0).flatten(), name = rs)\n",
    "#     dfs[rs] = s\n",
    "    \n",
    "# dfs = pd.DataFrame()\n",
    "# for rs in tqdm(RS):\n",
    "#     df = deviations.iloc[:1]\n",
    "#     s = pd.Series(dnn_model[str(arch) +'_Glam_dnn_MULTI_'+str(lr)+'_'+str(vs)+'_300_'+ str(rs)].predict(RGI, verbose=0).flatten(), name = rs)\n",
    "#     dfs[rs] = s\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Here we compute the average thickness for each RGI glacier\n",
    "\"\"\"\n",
    "RGI['avg predicted thickness'] = 'NaN'\n",
    "RGI_original = RGI.copy()\n",
    "for i in tqdm(dfs.index):\n",
    "    avg_predicted_thickness = np.sum(dfs.loc[i])/len(dfs.loc[i])\n",
    "#     print(i)\n",
    "#     print(np.sum(dfs.loc[i])/len(dfs.loc[i]))\n",
    "#     print('')\n",
    "#     break\n",
    "    RGI['avg predicted thickness'].loc[i] = avg_predicted_thickness\n",
    "\n",
    "RGI['predicted thickness std dev'] = 'NaN'\n",
    "for i in tqdm(dfs.index):\n",
    "    # step 1: calculate mean of numbers\n",
    "    avg_predicted_thickness = np.sum(dfs.loc[i])/len(dfs.loc[i])\n",
    "    \n",
    "    # step 2: subtract the mean from each, then square the result\n",
    "    \n",
    "    diff_sq = pd.Series()\n",
    "\n",
    "    for q in dfs:\n",
    "        \n",
    "        avg_predicted_thickness - dfs[q].loc[i]\n",
    "        step_2 = pd.Series((avg_predicted_thickness - dfs[q].loc[i])**2)\n",
    "        diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "    \n",
    "    # step 3: work out the mean of the squared differences    \n",
    "    mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "    # step 4: take the square root\n",
    "    prethick_std_dev = np.sqrt(mean_diff_sq)\n",
    "    RGI['predicted thickness std dev'].loc[i] = prethick_std_dev\n",
    "\n",
    "\n",
    "RGI['variance'] = (RGI['predicted thickness std dev'])**2\n",
    "\n",
    "RGI.to_csv('RGI_prethicked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('predicted RGI sum thickness (m)')\n",
    "print(sum(RGI['avg predicted thickness']))\n",
    "print('')\n",
    "print('predicted RGI thickness variance')\n",
    "print(sum(RGI['variance']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGI['predicted volume (km^3)'] = (RGI['avg predicted thickness']/1000) * RGI['Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ae8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code plots predictions against actual thickness. Not currently modified and will load\n",
    "EVERY SINGLE MODEL. DO NOT USE JUST YET\n",
    "\"\"\"\n",
    "for rs in RS:    \n",
    "    y = dnn_model['32-16-8_Glam_dnn_MULTI_0.01_0.2_300_'+ str(rs)].predict(test_features, verbose=0)\n",
    "    fig,ax=plt.subplots(1,1,figsize=(15,10))\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.plot(test_labels,y,'.')\n",
    "    plt.plot((0,300),(0,300),'-')\n",
    "    plt.xlabel('True Thickness (m)')\n",
    "    plt.ylabel('Model Thickness (m)')\n",
    "    ax.set_title('Random State ' +str(rs))\n",
    "    plt.xlim((0,300))\n",
    "    plt.ylim((0,300))\n",
    "    # plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99106e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load all histories\n",
    "print('Loading histories....')\n",
    "rootdir = 'sr2/'\n",
    "dnn_history = {}\n",
    "arch = deviations['layer architecture'].iloc[[0]]\n",
    "lr = deviations['learning rate'].iloc[[0]]\n",
    "vs = deviations['validation split'].iloc[[0]]\n",
    "print(arch)\n",
    "print(lr)\n",
    "print(vs)\n",
    "# for arch in tqdm(os.listdir(rootdir)):\n",
    "#     for folder in os.listdir(rootdir+arch):\n",
    "#         if 'MULTI' in folder:\n",
    "#             if 'dnn' in folder:\n",
    "\n",
    "#                 dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This cell plots each random state loss curve for the 25 random states for each run\n",
    "Also loads EVERY SINGLE MODEL currently and blows up the memory. Working on it.\n",
    "\"\"\"\n",
    "# for rs in RS:\n",
    "# for hist in dnn_history:    \n",
    "#     print(hist)\n",
    "#     fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     ax.set_title(hist)\n",
    "#     gl.plot_loss(dnn_history[hist])\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314c7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell plots the different variable loss curves to show individual variable models\n",
    "Not currently working because we have only loaded dnn_MULTI models\n",
    "\"\"\"\n",
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "fig.patch.set_facecolor('w')\n",
    "# ax.set_ylim([5,30])\n",
    "\n",
    "# gl.plot_loss(dnn_history['T_MULTI'])\n",
    "for i, variable_name in enumerate(list(train_features)):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    gl.plot_loss(dnn_history['glacier_'+ variable_name+ '_0.1_0.2_300_6'])\n",
    "#     ax.set_ylim([35,140])\n",
    "    ax.set_title(variable_name)\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d14cb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions.rename(columns = {'architecture':'layer architecture'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c051640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for rs in RS:\n",
    "    print(rs)\n",
    "    df = deviations.iloc[:1]\n",
    "    s = pd.Series(dnn_model['10-5_Glam_dnn_MULTI_0.01_0.2_300_'+str(rs)].predict(RGI).flatten(), name = rs)\n",
    "    dfs[rs] = s\n",
    "#     break\n",
    "#     dfs = dfs.assign(str(s))\n",
    "print(s)\n",
    "dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5031647",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in predictions[(predictions['learning rate'] == df['learning rate']) and (predictions['layer architecture'] == df['layer architecture'])]:\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_title('Layer architecture: ' + architecture)\n",
    "    ax.set_ylabel('prediction count')\n",
    "    ax.set_xlabel('thickness (m)')\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.hist(df['avg test thickness'])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations = deviations.sort_values('test mae avg')\n",
    "deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2ae64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains code to produce histograms of all the architectures different histories\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = deviations.iloc[:1]\n",
    "\n",
    "\n",
    "for rs in RS:\n",
    "    df = predictions[predictions['architecture'] == architecture]\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_title('Layer architecture: ' + architecture)\n",
    "    ax.set_ylabel('prediction count')\n",
    "    ax.set_xlabel('thickness (m)')\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.hist(df['avg test thickness'])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3251ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This cell contains code to produce histograms of all the architectures different histories\n",
    "# \"\"\"\n",
    "\n",
    "# for architecture in list(predictions['architecture'].unique()):\n",
    "#     df = predictions[predictions['architecture'] == architecture]\n",
    "#     fig,ax = plt.subplots()\n",
    "#     ax.set_title('Layer architecture: ' + architecture)\n",
    "#     ax.set_ylabel('prediction count')\n",
    "#     ax.set_xlabel('thickness (m)')\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     plt.hist(df['avg test thickness'])\n",
    "# # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6af61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell plots each random state loss curve for the 25 random states for each run\n",
    "Also loads EVERY SINGLE MODEL currently and blows up the memory. Working on it.\n",
    "\"\"\"\n",
    "# for rs in RS:\n",
    "for hist in dnn_history:    \n",
    "    fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "    fig.patch.set_facecolor('w')\n",
    "    ax.set_title(hist)\n",
    "    gl.plot_loss(dnn_history[hist])\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for modl in dnn_modl:    \n",
    "#     y = dnn_modl[modl].predict(test_features)\n",
    "#     fig,ax=plt.subplots(1,1,figsize=(15,10))\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     plt.plot(test_labels,y,'.')\n",
    "#     plt.plot((0,300),(0,300),'-')\n",
    "#     plt.xlabel('True Thickness (m)')\n",
    "#     plt.ylabel('Model Thickness (m)')\n",
    "#     plt.xlim((0,300))\n",
    "#     plt.ylim((0,300))\n",
    "#     # plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all histories\n",
    "print('Loading histories....')\n",
    "rootdir = 'sr2/'\n",
    "dnn_history = {}\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder:\n",
    "            if 'dnn' in folder:\n",
    "\n",
    "                dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "# dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc93aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95568cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586b2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a density plot of the most recent predictions made. Can easily be modified in a loop\n",
    "to show multiple random states and whatnot\n",
    "\"\"\"\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "sns.kdeplot(x = test_labels, y = y.flatten(),fill = True)\n",
    "plt.plot((0,300),(0,300),'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e735a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deviations.sort_values('test mae avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a741cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a7383",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # load all histories\n",
    "# qqq = deviations.sort_values('test mae avg')\n",
    "\n",
    "\n",
    "for rs in RS:\n",
    "#     dnn_histor+y[qqq['layer architecture']+\n",
    "#                 'Glam_dnn_history_MULTI_'+\n",
    "#                 qqq['learning rate']+\n",
    "#                 '_0.2_300_'+\n",
    "#                 rs]\n",
    "    gl.plot_loss(dnn_history[qqq['layer architecture'].iloc[0]+\n",
    "                '_Glam_dnn_history_MULTI_'+\n",
    "                qqq['learning rate'].iloc[0]+\n",
    "                '_0.2_300_'+\n",
    "                str(rs)])\n",
    "# print('Loading histories....')\n",
    "# rootdir = 'sr2/'\n",
    "# dnn_history = {}\n",
    "# for arch in tqdm(os.listdir(rootdir)):\n",
    "#     for folder in os.listdir(rootdir+arch):\n",
    "#         if 'MULTI' in folder and 'dnn' in folder:\n",
    "\n",
    "#             dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "# dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa78f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fee1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[qqq['layer architecture'].iloc[0]+\n",
    "                    '_Glam_dnn_history_MULTI_'+\n",
    "                    qqq['learning rate'].iloc[0]+\n",
    "                    '_0.2_300_'+\n",
    "                    str(rs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_single_variable(x,y,model_type,model_name,feature_name):\n",
    "#     y = model_type[model_name].predict(test_labels)\n",
    "#     plt.scatter(train_features[feature_name], train_labels, label='Data')\n",
    "#     plt.plot(x, y,'.', color='k', label='Predictions')\n",
    "#     plt.xlabel(feature_name)\n",
    "#     plt.ylabel('THICKNESS')\n",
    "#     plt.legend()\n",
    "#     plt.plot()\n",
    "\n",
    "# x = test_labels\n",
    "# for i, variable_name in enumerate(list(train_features)):\n",
    "#     ax = plt.subplot(2,2,i+1)\n",
    "#     model_name = (dataset.name \n",
    "#     + '_' \n",
    "#     + variable_name \n",
    "#     + '_' \n",
    "#     + str(lr) \n",
    "#     + '_' \n",
    "#     + str(vs) \n",
    "#     + '_' \n",
    "#     + str(ep))\n",
    "#     plot_single_variable(x,y,dnn_model, model_name,variable_name)\n",
    "# #     ax.set_ylim([35,140])\n",
    "# #     ax.set_title(variable_name)\n",
    "# #     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea1269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This cell contains code to load models and histories.\n",
    "# \"\"\"\n",
    "\n",
    "# # fix hyperparameters \n",
    "# lr = 0.1\n",
    "# vs = 0.2\n",
    "# # load models\n",
    "# print('Loading models....')\n",
    "\n",
    "# linear_model = {}\n",
    "# dnn_model = {}\n",
    "# # data is already split, however if other databases are used, then this line is needed\n",
    "# # (train_features, test_features, train_labels, test_labels) = gl.data_splitter(glacier)\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#         pth_mod \n",
    "#         + 'glacier' \n",
    "#         + '_linear_' \n",
    "#         + variable_name \n",
    "#         + '_' \n",
    "#         + str(lr)\n",
    "#         + '_'\n",
    "#         + str(vs)\n",
    "#         + '_'\n",
    "#         + str(ep)\n",
    "#         + '_'\n",
    "#         + str(rs)\n",
    "#         )\n",
    "\n",
    "#         linear_model[\n",
    "#             'glacier' \n",
    "#             + '_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = tf.keras.models.load_model(file_name)\n",
    "        \n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#         pth_mod \n",
    "#         + 'glacier' \n",
    "#         + '_dnn_' \n",
    "#         + variable_name \n",
    "#         + '_' \n",
    "#         + str(lr)\n",
    "#         + '_'\n",
    "#         + str(vs)\n",
    "#         + '_'\n",
    "#         + str(ep)\n",
    "#         + '_'\n",
    "#         + str(rs)\n",
    "#         )\n",
    "\n",
    "#         dnn_model[\n",
    "#             'glacier'\n",
    "#             + '_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = tf.keras.models.load_model(file_name)\n",
    "# print('Models loaded')\n",
    "\n",
    "# # load all histories\n",
    "# print('Loading histories....')\n",
    "# linear_history = {}\n",
    "# dnn_history = {}\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#             pth_res \n",
    "#             + 'glacier' \n",
    "#             +'_linear_history_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         )\n",
    "\n",
    "#         linear_history[\n",
    "#             'glacier' \n",
    "#             +'_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ]= pd.read_csv(file_name)\n",
    "\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#             pth_res \n",
    "#             + 'glacier_dnn_history_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         )\n",
    "\n",
    "#         dnn_history[\n",
    "#             'glacier' \n",
    "#             +'_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = pd.read_csv(file_name)\n",
    "# print('Histories loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03759a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This cell loads the loss histories from the original model with one dense layer of 64 nodes.\n",
    "From these histories I extracted the lowest and final loss value and plotted them.\n",
    "First is plotted the loss from using different learning rates with a fixed validation split. \n",
    "Then follows a plot of different validation splits using a fixed learning rate.\n",
    "\"\"\"\n",
    "\n",
    "# set up dictionaries\n",
    "loss = {}\n",
    "dnn_lr_history = {}\n",
    "loss['glacier_min_learn'] = pd.DataFrame()\n",
    "loss['glacier_fin_learn'] = pd.DataFrame()\n",
    "\n",
    "# loop to define and then load histories\n",
    "for lr in LR:\n",
    "    file_name = (\n",
    "    'sr/sr_64/'\n",
    "    + 'glacier_dnn_history_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    file = (\n",
    "    'glacier_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dnn_lr_history[\n",
    "    'glacier_MULTI_' \n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep)\n",
    "    ] = pd.read_csv(file_name)\n",
    "    \n",
    "    # find minimum and insert other model hyperparameters into table\n",
    "    m_loss = dnn_lr_history[file].min()\n",
    "    m_loss['learning rate'] = str(lr)\n",
    "    m_loss['validation split'] = str(vs)\n",
    "    m_loss['epochs'] = str(ep)\n",
    "    loss['glacier_min_learn'] = loss['glacier_min_learn'].append(m_loss,ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # find final and insert other model hyperparameters into table\n",
    "    f = dnn_lr_history[file].last_valid_index()\n",
    "    f_loss = dnn_lr_history[file].iloc[[f]]\n",
    "    f_loss['learning rate'] = str(lr)\n",
    "    f_loss['validation split'] = str(vs)\n",
    "    f_loss['epochs'] = str(ep)\n",
    "\n",
    "    loss['glacier_fin_learn'] = loss['glacier_fin_learn'].append(f_loss,ignore_index=True)\n",
    "\n",
    "loss['glacier_fin_learn'] = loss['glacier_fin_learn'].rename(columns = {\n",
    "    'loss':'loss_final',\n",
    "    'val_loss':'val_loss_final'\n",
    "})\n",
    "\n",
    "loss['glacier_min_learn'] = loss['glacier_min_learn'].rename(columns = {\n",
    "    'loss':'loss_minimum',\n",
    "    'val_loss':'val_loss_minimum'\n",
    "})\n",
    "    \n",
    "print('Results compiled')\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([5,30])\n",
    "# loss['glacier_min'].plot(x='validation split', y = ['loss_minimum', 'val_loss_minimum'], kind='bar',  ax=ax)\n",
    "# loss['glacier_fin'].plot(x='validation split', y = ['loss_final', 'val_loss_final'], kind='bar',  ax=ax)\n",
    "\n",
    "loss['glacier_fin_learn'].plot(x='learning rate', y = 'loss_final',color = 'blue',  ax=ax)\n",
    "loss['glacier_fin_learn'].plot(x='learning rate', y = 'val_loss_final',color = 'green', ax=ax)\n",
    "loss['glacier_min_learn'].plot(x='learning rate', y = 'loss_minimum', color = 'red', ax=ax)\n",
    "loss['glacier_min_learn'].plot(x='learning rate', y = 'val_loss_minimum',color = 'orange', ax=ax)\n",
    "ax.set_xlabel('Learning rate at fixed validation split = 0.2')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('GlaThiDa Glacier scale dataset multivariable regression hyperparameterization')\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# repeat above loop with fixed lr and varied vs\n",
    "lr = 0.1\n",
    "loss = {}\n",
    "dnn_vs_history = {}\n",
    "loss['glacier_min_valsplit'] = pd.DataFrame()\n",
    "loss['glacier_fin_valsplit'] = pd.DataFrame()\n",
    "for vs in VS:\n",
    "    file_name = (\n",
    "    'sr/sr_64/'\n",
    "    + 'glacier_dnn_history_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    file = (\n",
    "    'glacier_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dnn_lr_history[\n",
    "    'glacier_MULTI_' \n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep)\n",
    "    ] = pd.read_csv(file_name)\n",
    "    \n",
    "    \n",
    "    m_loss = dnn_lr_history[file].min()\n",
    "    m_loss['learning rate'] = str(lr)\n",
    "    m_loss['validation split'] = str(vs)\n",
    "    m_loss['epochs'] = str(ep)\n",
    "    loss['glacier_min_valsplit'] = loss['glacier_min_valsplit'].append(m_loss,ignore_index=True)\n",
    "\n",
    "    f = dnn_lr_history[file].last_valid_index()\n",
    "    f_loss = dnn_lr_history[file].iloc[[f]]\n",
    "    f_loss['learning rate'] = str(lr)\n",
    "    f_loss['validation split'] = str(vs)\n",
    "    f_loss['epochs'] = str(ep)\n",
    "\n",
    "    loss['glacier_fin_valsplit'] = loss['glacier_fin_valsplit'].append(f_loss,ignore_index=True)\n",
    "\n",
    "loss['glacier_fin_valsplit'] = loss['glacier_fin_valsplit'].rename(columns = {\n",
    "    'loss':'loss_final',\n",
    "    'val_loss':'val_loss_final'\n",
    "})\n",
    "\n",
    "loss['glacier_min_valsplit'] = loss['glacier_min_valsplit'].rename(columns = {\n",
    "    'loss':'loss_minimum',\n",
    "    'val_loss':'val_loss_minimum'\n",
    "})\n",
    "    \n",
    "print('Results compiled')\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([5,30])\n",
    "# loss['glacier_min'].plot(x='validation split', y = ['loss_minimum', 'val_loss_minimum'], kind='bar',  ax=ax)\n",
    "# loss['glacier_fin'].plot(x='validation split', y = ['loss_final', 'val_loss_final'], kind='bar',  ax=ax)\n",
    "\n",
    "loss['glacier_fin_valsplit'].plot(x='validation split', y = 'loss_final',color = 'blue',  ax=ax)\n",
    "loss['glacier_fin_valsplit'].plot(x='validation split', y = 'val_loss_final',color = 'green', ax=ax)\n",
    "loss['glacier_min_valsplit'].plot(x='validation split', y = 'loss_minimum', color = 'red', ax=ax)\n",
    "loss['glacier_min_valsplit'].plot(x='validation split', y = 'val_loss_minimum',color = 'orange', ax=ax)\n",
    "ax.set_xlabel('Validation splits with learning rate = 0.1')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('GlaThiDa Glacier scale dataset multivariable regression hyperparameterization')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell computes the true average thickness of the glaciers in use\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pth = '/data/fast1/glacierml/T_models/'\n",
    "T_lab = pd.read_csv(pth + 'T.csv', low_memory = False)\n",
    "T_lab = T_lab[[\n",
    "    'GlaThiDa_ID',\n",
    "    'LAT',\n",
    "    'LON',\n",
    "    'AREA',\n",
    "    'MEAN_SLOPE',\n",
    "    'MEAN_THICKNESS'\n",
    "]]\n",
    "T_lab = T_lab.dropna()\n",
    "\n",
    "tru_thickness = np.sum(T_lab['MEAN_THICKNESS']) / len(T_lab['MEAN_THICKNESS'])\n",
    "tru_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1983d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set definitions\n",
    "glacier = gl.data_loader(pth = '/data/fast1/glacierml/T_models/')\n",
    "# ,TT,TTT,TTTx,TTT_full\n",
    "# T_t = T.head()\n",
    "\n",
    "# rename thickness column in database\n",
    "gl.thickness_renamer(glacier)\n",
    "\n",
    "# split data for training and validation\n",
    "(train_features, test_features, train_labels, test_labels) = gl.data_splitter(glacier)\n",
    "\n",
    "# define model hyperparameters\n",
    "LR = np.logspace(-3,2,6)\n",
    "vs = 0.2\n",
    "VS = 0.1,0.15,0.2,0.25,0.3,0.35,0.4\n",
    "RS = range(0,25,1)\n",
    "ep = 300\n",
    "\n",
    "# name databases\n",
    "glacier.name = 'glacier'\n",
    "# T_t.name = 'T_t'\n",
    "# TT.name = 'band'\n",
    "# TTT.name = 'point'\n",
    "# TTTx.name = 'TTTx'\n",
    "# TTT_full.name = 'TTT_full'\n",
    "\n",
    "# old definitions, legacy code.\n",
    "\n",
    "# arch = '16-8'\n",
    "# pth_mod = 'sm/sm_' + arch + '/'\n",
    "# pth_res = 'sr/sr_' + arch + '/'\n",
    "\n",
    "\"\"\"\n",
    "Here we evaluate models and make predictions, then display the zults\n",
    "\"\"\"\n",
    "rootdir = 'sm/'\n",
    "# print(rootdir)\n",
    "dnn_model = {}\n",
    "predictions = pd.DataFrame()\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder and 'dnn' in folder:\n",
    "            \n",
    "            if '0.1' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.1'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'\n",
    "                \n",
    "            if '0.01' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features, verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.01'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'          \n",
    "            \n",
    "            if '0.001' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]            \n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.001'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'                \n",
    "                \n",
    "predictions.rename(columns = {0:'avg train thickness'},inplace = True)\n",
    "\n",
    "# these models are ridiculous, so we drop them.\n",
    "# idx = predictions.index[predictions['architecture']=='64']\n",
    "# predictions = predictions.drop(predictions.loc[idx].index)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we compute for each layer architecture avg mae, mae std dev, and\n",
    "prediction std dev.\n",
    "\"\"\"\n",
    "deviations = pd.DataFrame()\n",
    "for architecture in list(predictions['architecture'].unique()):\n",
    "    for learning_rate in list(predictions['learning rate'].unique()):\n",
    "        # define temp dataframe for calculations that contains only one layer architecture\n",
    "#         df = (predictions[predictions['architecture'] == architecture]) and (predictions[predictions['learning rate'] == str(learning_rate)])\n",
    "        df = predictions[(predictions['architecture'] == architecture) & (predictions['learning rate' ]== learning_rate)]\n",
    "#         break\n",
    "#         print(df)\n",
    "        # step 1: calculate mean of numbers\n",
    "        test_mae_mean = np.sum(df['test mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for test_mae in df['test mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((test_mae - test_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        test_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "       # repeat for train mae \n",
    "\n",
    "        # step 1: calculate mean of numbers\n",
    "        train_mae_mean = np.sum(df['train mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for train_mae in df['train mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((train_mae - train_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        train_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # repeat process for train thicknesses\n",
    "        thickness_train_mean = np.sum(df['avg train thickness']) / len(df)   \n",
    "        for thickness in df['avg train thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_train_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        train_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "        # repeat process for test thicknesses\n",
    "        thickness_test_mean = np.sum(df['avg test thickness']) / len(df)   \n",
    "        for thickness in df['avg test thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_test_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        test_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # turn the last number computed into a series so it may be appended to build the table.\n",
    "        # it will be dropped later, no worries.\n",
    "        test_thick_std_dev = pd.Series(test_thickness_std_dev)\n",
    "\n",
    "        deviations = deviations.append(test_thick_std_dev, ignore_index=True)\n",
    "        deviations.loc[deviations.index[-1], 'layer architecture']= architecture\n",
    "        deviations.loc[deviations.index[-1], 'model parameters'] = dnn_model[architecture + '_glacier_dnn_MULTI_0.1_0.2_300_0'].count_params()\n",
    "        deviations.loc[deviations.index[-1], 'learning rate']= learning_rate\n",
    "        deviations.loc[deviations.index[-1], 'validation split']= 0.2\n",
    "        deviations.loc[deviations.index[-1], 'test mae avg'] = test_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'train mae avg']= train_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'test mae std dev']= test_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train mae std dev']= train_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'test predicted thickness std dev']= test_thickness_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train predicted thickness std dev']= train_thickness_std_dev\n",
    "# bootstrapped ensembles for predicted column    \n",
    "#drop that appended line from earlier. Probably a better way to go about it\n",
    "deviations.drop(columns = {0},inplace = True)    \n",
    "deviations = deviations.dropna()\n",
    "deviations.sort_values('test mae avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc509540",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
